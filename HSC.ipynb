{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##This is a framework of patch-based hyperspectral classification\n","import os\n","import time\n","import random\n","import spectral\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report,recall_score,cohen_kappa_score,accuracy_score\n","from sklearn.preprocessing import minmax_scale\n","from scipy.io import loadmat\n","from tqdm.notebook import tqdm\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##hypeperameters and experimental settings\n","RANDOM_SEED=666\n","MODEL_NAME = 'CNN1D' ## \n","DATASET = 'IP'    ## PU  IP  SA  \n","TRAIN_RATE = 0.1  ## ratio of training data\n","VAL_RATE = 0.05    ## ratio of valuating data\n","EPOCH = 100    ##number of epoch\n","VAL_EPOCH = 1  ##interval of valuation\n","LR = 0.001    ##learning rate\n","WEIGHT_DECAY = 1e-6  \n","BATCH_SIZE = 64\n","DEVICE = 0  ##-1:CPU  0:cuda 0\n","N_PCA = 15  ## reserved PCA components   0:use origin data\n","NORM = True  ## normalization or not\n","PATCH_SIZE = 1 ## patchsize of input 3D cube   1:only spectral sequence\n","SAVE_PATH = f\"results\\\\{MODEL_NAME}\\\\{DATASET}\"\n","if not os.path.isdir(SAVE_PATH):\n","    os.makedirs(SAVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Set random seed for reproduction\n","random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed(RANDOM_SEED)\n","torch.cuda.manual_seed_all(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def loadData(name): ## customize data and return data label and class_name\n","    data_path = os.path.join(os.getcwd(),'dataset')\n","    if name == 'IP':\n","        data = loadmat(os.path.join(data_path, 'IndianPines\\\\Indian_pines_corrected.mat'))['indian_pines_corrected']\n","        labels = loadmat(os.path.join(data_path, 'IndianPines\\\\Indian_pines_gt.mat'))['indian_pines_gt']\n","        class_name = [ \"Alfalfa\", \"Corn-notill\", \"Corn-mintill\",\"Corn\", \"Grass-pasture\", \n","                       \"Grass-trees\",\"Grass-pasture-mowed\", \"Hay-windrowed\", \"Oats\",\"Soybean-notill\", \"Soybean-mintill\", \"Soybean-clean\",\"Wheat\", \"Woods\", \"Buildings-Grass-Trees-Drives\",\"Stone-Steel-Towers\"]\n","    elif name == 'SA':\n","        data = loadmat(os.path.join(data_path, 'Salinas\\\\Salinas_corrected.mat'))['salinas_corrected']\n","        labels = loadmat(os.path.join(data_path, 'Salinas\\\\Salinas_gt.mat'))['salinas_gt']\n","        class_name = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow',\n","                        'Fallow_rough_plow','Fallow_smooth','Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green','Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk','Vinyard_untrained','Vinyard_vertical']\n","    elif name == 'PU':\n","        data = loadmat(os.path.join(data_path, 'PaviaU\\\\PaviaU.mat'))['paviaU']\n","        labels = loadmat(os.path.join(data_path, 'PaviaU\\\\PaviaU_gt.mat'))['paviaU_gt']\n","        class_name = ['Asphalt', 'Meadows', 'Gravel', 'Trees','Painted metal sheets', 'Bare Soil', \n","                      'Bitumen','Self-Blocking Bricks', 'Shadows']\n","    return data, labels, class_name\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data,label,class_name = loadData(DATASET)\n","NUM_CLASS = label.max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## display HSI\n","rgb_view=spectral.imshow(data,(30,20,10),classes=label,title='RGB origin',figsize=(7,7))\n","gt_view = spectral.imshow(classes=label, title='GroundTruth',figsize=(7,7))\n","view = spectral.imshow(data,(30,20,10),classes=label,figsize=(7,7))\n","view.set_display_mode('overlay')\n","view.class_alpha = 0.5\n","view.set_title('Overlay')\n","spectral.save_rgb(f'results/{DATASET}_RGB_origin.jpg',data,(30,20,10))\n","spectral.save_rgb(f'results/{DATASET}_gt.jpg',label,colors = spectral.spy_colors)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## show 3D cube\n","# %matplotlib auto\n","# spectral.view_cube(data,(30,20,10))   ## depends on wxpython and pyopengl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def applyPCA(X, numComponents=15, norm = True):\n","    \"\"\"PCA and processing\n","    Args:\n","        X (ndarray M*N*C): data needs DR\n","        numComponents (int, optional):  number of reserved components(Defaults to 15, 0 for no PCA).\n","        norm: normalization or not\n","    Returns:\n","        newX: processed data\n","        pca: \n","    \"\"\"\n","    if numComponents == 0:\n","        newX = X\n","    else:\n","        newX = np.reshape(X, (-1, X.shape[2]))\n","        pca = PCA(n_components=numComponents)   ##PCA and normalization\n","        newX = pca.fit_transform(newX)\n","    if norm:\n","        newX = minmax_scale(newX, axis=1)\n","    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n","    return newX, newX.shape[2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data,N_PCA = applyPCA(data, N_PCA, NORM)\n","data.shape,N_PCA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sample_gt(gt, train_rate):\n","    \"\"\" generate training gt for training dataset\n","    Args:\n","        gt (ndarray): full classmap\n","        train_rate (float): ratio of training dataset\n","    Returns:\n","        train_gt(ndarray): classmap of training data\n","        test_gt(ndarray): classmap of test data\n","    \"\"\"\n","    indices = np.nonzero(gt)  ##([x1,x2,...],[y1,y2,...])\n","    X = list(zip(*indices))  ## X=[(x1,y1),(x2,y2),...] location of pixels\n","    y = gt[indices].ravel()\n","    train_gt = np.zeros_like(gt)\n","    test_gt = np.zeros_like(gt)\n","    if train_rate > 1:\n","       train_rate = int(train_rate)\n","    train_indices, test_indices = train_test_split(X, train_size=train_rate, stratify=y, random_state=100)\n","    train_indices = [t for t in zip(*train_indices)]   ##[[x1,x2,...],[y1,y2,...]]\n","    test_indices = [t for t in zip(*test_indices)]\n","    train_gt[tuple(train_indices)] = gt[tuple(train_indices)]\n","    test_gt[tuple(test_indices)] = gt[tuple(test_indices)]\n","    \n","    return train_gt, test_gt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_gt, test_gt = sample_gt(label,TRAIN_RATE)\n","val_gt,test_gt = sample_gt(test_gt,VAL_RATE/(1-TRAIN_RATE))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## display sampling info\n","sample_report = f\"{'class': ^25}{'train_num':^10}{'val_num': ^10}{'test_num': ^10}{'total': ^10}\\n\"\n","for i in np.unique(label):\n","    if i == 0: continue\n","    sample_report += f\"{class_name[i-1]: ^25}{(train_gt==i).sum(): ^10}{(val_gt==i).sum(): ^10}{(test_gt==i).sum(): ^10}{(label==i).sum(): ^10}\\n\"\n","sample_report += f\"{'total': ^25}{np.count_nonzero(train_gt): ^10}{np.count_nonzero(val_gt): ^10}{np.count_nonzero(test_gt): ^10}{np.count_nonzero(label): ^10}\"\n","print(sample_report)\n","spectral.imshow(classes=train_gt, title='train_gt')\n","spectral.imshow(classes=val_gt, title='val_gt')\n","spectral.imshow(classes=test_gt, title='test_gt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PatchSet(Dataset):\n","    \"\"\" Generate 3D patch from hyperspectral dataset \"\"\"\n","    def __init__(self, data, gt, patch_size, is_pred=False):\n","        \"\"\"\n","        Args:\n","            data: 3D hyperspectral image\n","            gt: 2D array of labels\n","            patch_size: int, size of the 3D patch\n","            is_pred: bool, create data without label for prediction (default False) \n","        \"\"\"\n","        super(PatchSet, self).__init__()\n","        self.is_pred = is_pred\n","        self.patch_size = patch_size\n","        p = self.patch_size // 2\n","        self.data = np.pad(data,((p,p),(p,p),(0,0)),'constant',constant_values = 0)\n","        if is_pred:\n","            gt = np.ones_like(gt)\n","        self.label = np.pad(gt,(p,p),'constant',constant_values = 0)\n","        x_pos, y_pos = np.nonzero(gt)\n","        x_pos, y_pos = x_pos + p, y_pos + p   ##indices after padding\n","        self.indices = np.array([(x,y) for x,y in zip(x_pos, y_pos)])\n","        if not is_pred:\n","            np.random.shuffle(self.indices)\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, i):\n","        x, y = self.indices[i]\n","        x1, y1 = x - self.patch_size // 2, y - self.patch_size // 2\n","        x2, y2 = x1 + self.patch_size, y1 + self.patch_size\n","        data = self.data[x1:x2, y1:y2]\n","        label = self.label[x, y]\n","        data = np.asarray(data, dtype='float32').transpose((2, 0, 1))\n","        label = np.asarray(label, dtype='int64')\n","        data = torch.from_numpy(data)\n","        label = torch.from_numpy(label)\n","        if self.is_pred:\n","            return data\n","        else: return data, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##create dataset and dataloader\n","train_data = PatchSet(data, train_gt, PATCH_SIZE)\n","val_data = PatchSet(data, val_gt, PATCH_SIZE)\n","all_data = PatchSet(data, label, PATCH_SIZE,is_pred = True)\n","train_loader = DataLoader(train_data,BATCH_SIZE,shuffle= True)\n","val_loader = DataLoader(val_data,BATCH_SIZE,shuffle= True)\n","all_loader = DataLoader(all_data,BATCH_SIZE,shuffle= False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["d,g=train_data.__getitem__(0)\n","d.shape,g"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## establish model\n","## write you model here and use follow frame\n","\n","# # class MODEL(nn.Module):\n","\n","# #     def __init__(self, input_channels, n_classes, *args):\n","# #         super(MODEL, self).__init__()\n","        \n","# #     def forward(self, x):\n","# #         return x\n","\n","## we use CNN1D as example\n","import math\n","from torch.nn import init\n","class MODEL(nn.Module):\n","    \"\"\"\n","    Deep Convolutional Neural Networks for Hyperspectral Image Classification\n","    Wei Hu, Yangyu Huang, Li Wei, Fan Zhang and Hengchao Li\n","    Journal of Sensors, Volume 2015 (2015)\n","    https://www.hindawi.com/journals/js/2015/258619/\n","    \"\"\"\n","    @staticmethod\n","    def weight_init(m):\n","        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","            init.uniform_(m.weight, -0.05, 0.05)\n","            init.zeros_(m.bias)\n","\n","    def _get_final_flattened_size(self):\n","        with torch.no_grad():\n","            x = torch.zeros(1, 1, self.input_channels)\n","            x = self.pool(self.conv(x))\n","        return x.numel() \n","\n","    def __init__(self, input_channels, n_classes, kernel_size=None, pool_size=None):\n","        super(MODEL, self).__init__()\n","        if kernel_size is None:\n","           kernel_size = math.ceil(input_channels / 9)\n","        if pool_size is None:\n","           pool_size = math.ceil(kernel_size / 5)\n","        self.input_channels = input_channels\n","\n","        self.conv = nn.Conv1d(1, 20, kernel_size)\n","        self.pool = nn.MaxPool1d(pool_size)\n","        self.features_size = self._get_final_flattened_size()\n","        self.fc1 = nn.Linear(self.features_size, 100)\n","        self.fc2 = nn.Linear(100, n_classes)\n","        self.apply(self.weight_init)\n","\n","    def forward(self, x):\n","        x = x.squeeze(dim=-1).squeeze(dim=-1)\n","        x = x.unsqueeze(1)\n","        x = self.conv(x)\n","        x = torch.tanh(self.pool(x))\n","        x = x.view(-1, self.features_size)\n","        x = torch.tanh(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","##display network details\n","net = MODEL(N_PCA,n_classes=NUM_CLASS)\n","summary(net, input_size=(1,N_PCA,PATCH_SIZE,PATCH_SIZE),col_names=['num_params','kernel_size','mult_adds','input_size','output_size'],col_width=10,row_settings=['var_names'],depth=4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## training the model\n","device = torch.device(DEVICE if DEVICE>=0 and torch.cuda.is_available() else 'cpu')\n","loss_list = []\n","acc_list = []\n","val_acc_list = []\n","val_epoch_list = []\n","model = MODEL(N_PCA,n_classes=NUM_CLASS) ##modify to you model \n","\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(),LR,weight_decay=WEIGHT_DECAY)\n","loss_func = nn.CrossEntropyLoss()\n","batch_num = len(train_loader)\n","train_num = train_loader.dataset.__len__()\n","val_num = val_loader.dataset.__len__()\n","\n","train_st = time.time()\n","try:\n","    for e in tqdm(range(EPOCH), desc=\"Training:\"):\n","        model.train()\n","        avg_loss = 0.\n","        train_acc = 0\n","        for batch_idx, (data, target) in tqdm(enumerate(train_loader),total=batch_num):\n","            data,target = data.to(device),target.to(device)\n","            optimizer.zero_grad()\n","            out = model(data)\n","            target = target - 1  ## class 0 in out is class 1 in target\n","            loss = loss_func(out,target)\n","            loss.backward()\n","            optimizer.step()\n","            avg_loss += loss.item()\n","            _,pred = torch.max(out,dim=1)\n","            train_acc += (pred == target).sum().item()\n","        loss_list.append(avg_loss/train_num)\n","        acc_list.append(train_acc/train_num)\n","        print(f\"epoch {e}/{EPOCH} loss:{loss_list[e]}  acc:{acc_list[e]}\")\n","        ## valuation\n","        if (e+1)%VAL_EPOCH == 0 or (e+1)==EPOCH:\n","            val_acc =0\n","            model.eval()\n","            for batch_idx, (data, target) in tqdm(enumerate(val_loader),total=len(val_loader)):\n","                data,target = data.to(device),target.to(device)\n","                out = model(data)\n","                target = target - 1  ## class 0 in out is class 1 in target\n","                _,pred = torch.max(out,dim=1)\n","                val_acc += (pred == target).sum().item()\n","            val_acc_list.append(val_acc/val_num)\n","            val_epoch_list.append(e)\n","            print(f\"epoch {e}/{EPOCH}  val_acc:{val_acc_list[-1]}\")\n","            save_name = os.path.join(SAVE_PATH, f\"epoch_{e}_acc_{val_acc_list[-1]:.4f}.pth\")\n","            torch.save(model.state_dict(),save_name)\n","except Exception as exc:\n","    print(exc)\n","finally: \n","    print(f'Stop in epoch {e}')\n","train_time = time.time()-train_st\n","print(f\"training time: {train_time}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##display loss and acc\n","fig1 = plt.figure()\n","fig2 = plt.figure()\n","ax1 = fig1.add_subplot(1,1,1)\n","ax2 = fig2.add_subplot(1,1,1)\n","ax1.plot(np.arange(e+1),loss_list)\n","ax1.set_title('loss')\n","ax1.set_xlabel('epoch')\n","ax2.plot(np.arange(e+1),acc_list,label = 'train_acc')\n","ax2.plot(val_epoch_list,val_acc_list,label = 'val_acc')\n","ax2.set_title('acc')\n","ax2.set_xlabel('epoch')\n","ax2.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## get best model path and del other models\n","def get_best_model(acc_list, epoch_list, save_path):\n","    \"\"\"get best model path by valuation list\n","    Args:\n","        acc_list (list): list of valuation accuracy\n","        epoch_list (list): list of valuation epoch\n","        save_path (str): path of save dir\n","    Returns:\n","        best_model_path: path of best model\n","    \"\"\"\n","    acc_list = np.array(acc_list)\n","    epoch_list = np.array(epoch_list)\n","    best_index = np.argwhere(acc_list==np.max(acc_list))[-1].item()\n","    best_epoch = epoch_list[best_index]\n","    best_acc = acc_list[best_index]\n","    file_name = f\"epoch_{best_epoch}_acc_{best_acc:.4f}.pth\"\n","    best_model_path=os.path.join(save_path, file_name)\n","    print(f\"best model:{file_name}\")\n","    ##del save model except best model\n","    for f in os.listdir(save_path):\n","        if f[-3:]=='pth' and os.path.join(save_path,f)!=best_model_path:\n","            os.remove(os.path.join(save_path,f))\n","    return best_model_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## inferring the whole image\n","##load best model\n","best_model_path = get_best_model(val_acc_list,val_epoch_list,SAVE_PATH)\n","\n","best_model = MODEL(N_PCA,n_classes=NUM_CLASS)  ## modify to you model\n","\n","best_model.load_state_dict(torch.load(best_model_path))\n","## inference\n","best_model.to(device)\n","best_model.eval()\n","pred_map = []\n","infer_st = time.time() \n","for batch_idx, data in tqdm(enumerate(all_loader),total=len(all_loader)):\n","    data = data.to(device)\n","    target = best_model(data)\n","    _, pred = torch.max(target, dim = 1)\n","    pred_map += [np.array(pred.detach().cpu() + 1)]   ## class 0 in pred_map is class 1 in gt\n","infer_time = time.time() - infer_st\n","print(f\"inferring time: {infer_time}\")\n","pred_map = np.asarray(np.hstack(pred_map),dtype=np.uint8).reshape(label.shape[0],label.shape[1])\n","spectral.imshow(classes=pred_map,title='prediction',figsize=(7,7))\n","spectral.imshow(classes=pred_map*(label!=0),title='prediction_masked',figsize=(7,7))\n","spectral.save_rgb(os.path.join(SAVE_PATH,f\"prediction.jpg\"),pred_map,colors = spectral.spy_colors)\n","spectral.save_rgb(os.path.join(SAVE_PATH,f\"prediction_masked.jpg\"),pred_map*(label!=0),colors = spectral.spy_colors)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## classfication report\n","test_pred = pred_map[test_gt!=0]\n","test_true = test_gt[test_gt!=0]\n","\n","OA = accuracy_score(test_true,test_pred)\n","AA = recall_score(test_true,test_pred,average='macro')\n","kappa = cohen_kappa_score(test_true,test_pred)\n","report_log = F\"OA: {OA}\\nAA: {AA}\\nKappa: {kappa}\\n\"\n","report_log += f\"training time: {train_time}\\ninferring time: {infer_time}\\n\"\n","report_log += classification_report(test_true,test_pred,target_names=class_name,digits=4)\n","print(report_log)\n","fp = open(os.path.join(SAVE_PATH,'classfication_report.txt'),'w+')\n","fp.writelines(report_log)\n","fp.close()"]}],"metadata":{"interpreter":{"hash":"cf59b85677ef8bbc27ae54bd5fffd1227219dbab0c241877bdfafd2c89938c2d"},"kernelspec":{"display_name":"Python 3.7.11 ('pytorch-gpu')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
